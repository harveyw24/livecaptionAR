{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = './result/'       # The path that the result images will be saved\n",
    "VIDEO_PATH = './dataset/'       # Dataset path\n",
    "LOG_PATH = 'log.txt'            # The path for the working log file\n",
    "LIP_MARGIN = 0.3                # Marginal rate for lip-only image.\n",
    "RESIZE = (96,96)                # Final image size\n",
    "logfile = open(LOG_PATH,'w')\n",
    "# Face detector and landmark detector\n",
    "face_detector = dlib.get_frontal_face_detector()   \n",
    "landmark_detector = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\t# Landmark detector path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_list(shape):\n",
    "\tcoords = []\n",
    "\tfor i in range(0, 68):\n",
    "\t\tcoords.append((shape.part(i).x, shape.part(i).y))\n",
    "\treturn coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = os.listdir(VIDEO_PATH)     # Read video list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid_name in video_list:                 # Iterate on video files\n",
    "    vid_path = VIDEO_PATH + vid_name\n",
    "    vid = cv2.VideoCapture(vid_path)       # Read video\n",
    "\n",
    "    # Parse into frames \n",
    "    frame_buffer = []               # A list to hold frame images\n",
    "    frame_buffer_color = []         # A list to hold original frame images\n",
    "    while(True):\n",
    "        success, frame = vid.read()                # Read frame\n",
    "        if not success:\n",
    "            break                           # Break if no frame to read left\n",
    "        gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)   # Convert image into grayscale\n",
    "        frame_buffer.append(gray)                  # Add image to the frame buffer\n",
    "        frame_buffer_color.append(frame)\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain face landmark information\n",
    "landmark_buffer = []        # A list to hold face landmark information\n",
    "for (i, image) in enumerate(frame_buffer):          # Iterate on frame buffer\n",
    "    face_rects = face_detector(image,1)             # Detect face\n",
    "    if len(face_rects) < 1:                 # No face detected\n",
    "        print(\"No face detected: \",vid_path)\n",
    "        logfile.write(vid_path + \" : No face detected \\r\\n\")\n",
    "        break\n",
    "    if len(face_rects) > 1:                  # Too many face detected\n",
    "        print(\"Too many face: \",vid_path)\n",
    "        logfile.write(vid_path + \" : Too many face detected \\r\\n\")\n",
    "        break\n",
    "    rect = face_rects[0]                    # Proper number of face\n",
    "    landmark = landmark_detector(image, rect)   # Detect face landmarks\n",
    "    landmark = shape_to_list(landmark)\n",
    "    landmark_buffer.append(landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmark(image):\n",
    "    face_rects = face_detector(image,1)             # Detect face\n",
    "    if len(face_rects) < 1:                 # No face detected\n",
    "        print(\"No face detected: \",vid_path)\n",
    "        logfile.write(vid_path + \" : No face detected \\r\\n\")\n",
    "        return\n",
    "    if len(face_rects) > 1:                  # Too many face detected\n",
    "        print(\"Too many face: \",vid_path)\n",
    "        logfile.write(vid_path + \" : Too many face detected \\r\\n\")\n",
    "        return\n",
    "    rect = face_rects[0]                    # Proper number of face\n",
    "    landmark = landmark_detector(image, rect)   # Detect face landmarks\n",
    "    landmark = shape_to_list(landmark)\n",
    "    # landmark_buffer.append(landmark)\n",
    "    return landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, TimeoutError\n",
    "# from multiprocessing.dummy import Pool\n",
    "import time\n",
    "import os\n",
    "\n",
    "with Pool(processes = 10) as pool:\n",
    "    landmark_buffer2 = pool.map(detect_landmark, frame_buffer)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "if len(landmark_buffer) != len(landmark_buffer2):\n",
    "    print(\"ERROR: lengths not equal\")\n",
    "else:\n",
    "    for i in range(len(landmark_buffer)):\n",
    "        if landmark_buffer[i] != landmark_buffer2[i]:\n",
    "            print(\"ERROR: buffers don't match\")\n",
    "            break\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images\n",
    "cropped_buffer = []\n",
    "for (i,landmark) in enumerate(landmark_buffer):\n",
    "    lip_landmark = landmark[48:68]                                          # Landmark corresponding to lip\n",
    "    lip_x = sorted(lip_landmark,key = lambda pointx: pointx[0])             # Lip landmark sorted for determining lip region\n",
    "    lip_y = sorted(lip_landmark, key = lambda pointy: pointy[1])\n",
    "    x_add = int((-lip_x[0][0]+lip_x[-1][0])*LIP_MARGIN)                     # Determine Margins for lip-only image\n",
    "    y_add = int((-lip_y[0][1]+lip_y[-1][1])*LIP_MARGIN)\n",
    "    crop_pos = (lip_x[0][0]-x_add, lip_x[-1][0]+x_add, lip_y[0][1]-y_add, lip_y[-1][1]+y_add)   # Crop image\n",
    "    cropped = frame_buffer_color[i][crop_pos[2]:crop_pos[3],crop_pos[0]:crop_pos[1]]\n",
    "    cropped = cv2.resize(cropped,(RESIZE[0],RESIZE[1]),interpolation=cv2.INTER_CUBIC)        # Resize\n",
    "    cropped_buffer.append(cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "directory = RESULT_PATH + vid_name + \"/\"\n",
    "for (i,image) in enumerate(cropped_buffer):\n",
    "    if not os.path.exists(directory):           # If the directory not exists, make it.\n",
    "        os.makedirs(directory)\n",
    "    cv2.imwrite(directory + \"%d\"%(i+1) + \".jpg\", image)     # Write lip image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, image2, from './result/angel_clip_mp4_compressed.mp4/%d.jpg':\n",
      "  Duration: 00:00:10.08, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: mjpeg (Baseline), yuvj420p(pc, bt470bg/unknown/unknown), 96x96 [SAR 1:1 DAR 1:1], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x561a563db440] using SAR=1/1\n",
      "[libx264 @ 0x561a563db440] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x561a563db440] profile High, level 1.0\n",
      "[libx264 @ 0x561a563db440] 264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 96x96 [SAR 1:1 DAR 1:1], q=-1--1, 25 fps, 12800 tbn, 25 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  252 fps=0.0 q=-1.0 Lsize=      56kB time=00:00:09.96 bitrate=  46.4kbits/s speed=47.1x    \n",
      "video:53kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 7.029730%\n",
      "[libx264 @ 0x561a563db440] frame I:3     Avg QP:23.60  size:   755\n",
      "[libx264 @ 0x561a563db440] frame P:87    Avg QP:25.88  size:   360\n",
      "[libx264 @ 0x561a563db440] frame B:162   Avg QP:28.88  size:   122\n",
      "[libx264 @ 0x561a563db440] consecutive B-frames:  9.1% 12.7%  8.3% 69.8%\n",
      "[libx264 @ 0x561a563db440] mb I  I16..4:  1.9% 95.4%  2.8%\n",
      "[libx264 @ 0x561a563db440] mb P  I16..4:  0.2% 11.4%  1.6%  P16..4: 38.7% 31.0% 11.5%  0.0%  0.0%    skip: 5.7%\n",
      "[libx264 @ 0x561a563db440] mb B  I16..4:  0.0%  0.5%  0.1%  B16..8: 51.1% 15.2%  2.5%  direct: 2.9%  skip:27.6%  L0:44.5% L1:47.0% BI: 8.5%\n",
      "[libx264 @ 0x561a563db440] 8x8 transform intra:88.0% inter:78.8%\n",
      "[libx264 @ 0x561a563db440] coded y,uvDC,uvAC intra: 91.0% 88.5% 33.8% inter: 26.0% 11.3% 0.1%\n",
      "[libx264 @ 0x561a563db440] i16 v,h,dc,p: 56%  0%  0% 44%\n",
      "[libx264 @ 0x561a563db440] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 15% 12%  4% 12% 11% 11%  9%  9%\n",
      "[libx264 @ 0x561a563db440] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 11% 15%  3% 14% 12% 10%  6%  6%\n",
      "[libx264 @ 0x561a563db440] i8c dc,h,v,p: 51% 17% 20% 12%\n",
      "[libx264 @ 0x561a563db440] Weighted P-Frames: Y:21.8% UV:5.7%\n",
      "[libx264 @ 0x561a563db440] ref P L0: 55.1% 21.3% 14.4%  7.5%  1.6%\n",
      "[libx264 @ 0x561a563db440] ref B L0: 86.3% 10.9%  2.8%\n",
      "[libx264 @ 0x561a563db440] ref B L1: 92.6%  7.4%\n",
      "[libx264 @ 0x561a563db440] kb/s:42.30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command = f'ffmpeg -i {directory}%d.jpg output.mp4'\n",
    "subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_path = \"dataset/angel_clip_mp4_compressed.mp4\"\n",
    "output_video_path = \"angel_output.mp4\"\n",
    "face_predictor_path = \"shape_predictor_68_face_landmarks.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from multiprocessing import Pool\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def shape_to_list(shape):\n",
    "\tcoords = []\n",
    "\tfor i in range(0, 68):\n",
    "\t\tcoords.append((shape.part(i).x, shape.part(i).y))\n",
    "\treturn coords\n",
    "\n",
    "# Obtain face landmark information\n",
    "def detect_landmark(image, landmark_detector):\n",
    "    \n",
    "    face_rects = face_detector(image,1)             # Detect face\n",
    "    if len(face_rects) < 1:                 # No face detected\n",
    "        print(\"No face detected\")\n",
    "        return\n",
    "    if len(face_rects) > 1:                  # Too many face detected\n",
    "        print(\"Too many faces\")\n",
    "        return\n",
    "    rect = face_rects[0]                    # Proper number of face\n",
    "    landmark = landmark_detector(image, rect)   # Detect face landmarks\n",
    "    landmark = shape_to_list(landmark)\n",
    "    return landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmark_unpack(args):\n",
    "    return detect_landmark(args[0], args[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_video(input_video_path, output_video_path, face_predictor_path):\n",
    "\n",
    "LIP_MARGIN = 0.3                # Marginal rate for lip-only image.\n",
    "RESIZE = (96,96)                # Final image size\n",
    "\n",
    "vid = cv2.VideoCapture(input_video_path)       # Read video\n",
    "\n",
    "# Parse into frames\n",
    "frame_buffer = []               # A list to hold frame images\n",
    "frame_buffer_color = []         # A list to hold original frame images\n",
    "while(True):\n",
    "    success, frame = vid.read()                # Read frame\n",
    "    if not success:\n",
    "        break                           # Break if no frame to read left\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)   # Convert image into grayscale\n",
    "    frame_buffer.append(gray)                  # Add image to the frame buffer\n",
    "    frame_buffer_color.append(frame)\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "landmark_detector = dlib.shape_predictor(face_predictor_path)\n",
    "\n",
    "with Pool(processes = 10) as pool:\n",
    "    landmark_buffer = pool.starmap(detect_landmark, zip(frame_buffer, repeat(landmark_detector)))\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images\n",
    "cropped_buffer = []\n",
    "for (i,landmark) in enumerate(landmark_buffer):\n",
    "    lip_landmark = landmark[48:68]                                          # Landmark corresponding to lip\n",
    "    lip_x = sorted(lip_landmark,key = lambda pointx: pointx[0])             # Lip landmark sorted for determining lip region\n",
    "    lip_y = sorted(lip_landmark, key = lambda pointy: pointy[1])\n",
    "    x_add = int((-lip_x[0][0]+lip_x[-1][0])*LIP_MARGIN)                     # Determine Margins for lip-only image\n",
    "    y_add = int((-lip_y[0][1]+lip_y[-1][1])*LIP_MARGIN)\n",
    "    crop_pos = (lip_x[0][0]-x_add, lip_x[-1][0]+x_add, lip_y[0][1]-y_add, lip_y[-1][1]+y_add)   # Crop image\n",
    "    cropped = frame_buffer_color[i][crop_pos[2]:crop_pos[3],crop_pos[0]:crop_pos[1]]\n",
    "    cropped = cv2.resize(cropped,(RESIZE[0],RESIZE[1]),interpolation=cv2.INTER_CUBIC)        # Resize\n",
    "    cropped_buffer.append(cropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save result to temp folder\n",
    "with tempfile.TemporaryDirectory() as tempdir:\n",
    "    for (i,image) in enumerate(cropped_buffer):\n",
    "        cv2.imwrite(str(tempdir) + \"%d\"%(i+1) + \".jpg\", image)     # Write lip image\n",
    "\n",
    "    # Save video\n",
    "    command = f'ffmpeg -i {str(tempdir)}%d.jpg {output_video_path}'\n",
    "    subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: cannot open TemporaryDirectory: No such file\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rmdir: path should be string, bytes or os.PathLike, not TemporaryDirectory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m process_video(\u001b[39m\"\u001b[39;49m\u001b[39mdataset/angel_clip_mp4_compressed.mp4\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mangel_output.mp4\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mshape_predictor_68_face_landmarks.dat\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [2], line 48\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(input_video_path, output_video_path, face_predictor_path)\u001b[0m\n\u001b[1;32m     45\u001b[0m command \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mffmpeg -i \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(tempdir)\u001b[39m}\u001b[39;00m\u001b[39m%d.jpg \u001b[39m\u001b[39m{\u001b[39;00moutput_video_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     46\u001b[0m subprocess\u001b[39m.\u001b[39mcall(command, shell\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 48\u001b[0m os\u001b[39m.\u001b[39;49mremovedirs(tempdir)\n",
      "File \u001b[0;32m~/miniconda3/envs/avhubert/lib/python3.8/os.py:241\u001b[0m, in \u001b[0;36mremovedirs\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremovedirs\u001b[39m(name):\n\u001b[1;32m    231\u001b[0m     \u001b[39m\"\"\"removedirs(name)\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[39m    Super-rmdir; remove a leaf directory and all empty intermediate\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     rmdir(name)\n\u001b[1;32m    242\u001b[0m     head, tail \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39msplit(name)\n\u001b[1;32m    243\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tail:\n",
      "\u001b[0;31mTypeError\u001b[0m: rmdir: path should be string, bytes or os.PathLike, not TemporaryDirectory"
     ]
    }
   ],
   "source": [
    "process_video(\"dataset/angel_clip_mp4_compressed.mp4\", \"angel_output.mp4\", \"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "from functools import partial\n",
    "from multiprocessing.dummy import Pool\n",
    "\n",
    "class ROIExtractor:\n",
    "    def __init__(self, face_predictor_path):\n",
    "        self.landmark_detector = dlib.shape_predictor(face_predictor_path)\n",
    "        self.face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    def shape_to_list(self,  shape):\n",
    "        coords = []\n",
    "        for i in range(0, 68):\n",
    "            coords.append((shape.part(i).x, shape.part(i).y))\n",
    "        return coords\n",
    "\n",
    "    # Obtain face landmark information\n",
    "    def detect_landmark(self, image):\n",
    "        face_rects = self.face_detector(image,1)             # Detect face\n",
    "        if len(face_rects) < 1:                 # No face detected\n",
    "            print(\"No face detected\")\n",
    "            return\n",
    "        if len(face_rects) > 1:                  # Too many face detected\n",
    "            print(\"Too many faces\")\n",
    "            return\n",
    "        rect = face_rects[0]                    # Proper number of face\n",
    "        landmark = self.landmark_detector(image, rect)   # Detect face landmarks\n",
    "        landmark = self.shape_to_list(landmark)\n",
    "        # landmark_buffer.append(landmark)\n",
    "        return landmark\n",
    "\n",
    "    def process_video(self, input_video_path, output_video_path):\n",
    "        LIP_MARGIN = 0.3                # Marginal rate for lip-only image.\n",
    "        RESIZE = (96,96)                # Final image size\n",
    "\n",
    "        vid = cv2.VideoCapture(input_video_path)       # Read video\n",
    "\n",
    "        # Parse into frames\n",
    "        frame_buffer = []               # A list to hold frame images\n",
    "        frame_buffer_color = []         # A list to hold original frame images\n",
    "        while(True):\n",
    "            success, frame = vid.read()                # Read frame\n",
    "            if not success:\n",
    "                break                           # Break if no frame to read left\n",
    "            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)   # Convert image into grayscale\n",
    "            frame_buffer.append(gray)                  # Add image to the frame buffer\n",
    "            frame_buffer_color.append(frame)\n",
    "        vid.release()\n",
    "\n",
    "        with Pool(processes = 10) as pool:\n",
    "            landmark_buffer = pool.map(self.detect_landmark, frame_buffer)\n",
    "        \n",
    "        print(\"done\")\n",
    "\n",
    "        # Crop images\n",
    "        cropped_buffer = []\n",
    "        for (i,landmark) in enumerate(landmark_buffer):\n",
    "            lip_landmark = landmark[48:68]                                          # Landmark corresponding to lip\n",
    "            lip_x = sorted(lip_landmark,key = lambda pointx: pointx[0])             # Lip landmark sorted for determining lip region\n",
    "            lip_y = sorted(lip_landmark, key = lambda pointy: pointy[1])\n",
    "            x_add = int((-lip_x[0][0]+lip_x[-1][0])*LIP_MARGIN)                     # Determine Margins for lip-only image\n",
    "            y_add = int((-lip_y[0][1]+lip_y[-1][1])*LIP_MARGIN)\n",
    "            crop_pos = (lip_x[0][0]-x_add, lip_x[-1][0]+x_add, lip_y[0][1]-y_add, lip_y[-1][1]+y_add)   # Crop image\n",
    "            cropped = frame_buffer_color[i][crop_pos[2]:crop_pos[3],crop_pos[0]:crop_pos[1]]\n",
    "            cropped = cv2.resize(cropped,(RESIZE[0],RESIZE[1]),interpolation=cv2.INTER_CUBIC)        # Resize\n",
    "            cropped_buffer.append(cropped)\n",
    "\n",
    "        # Save result to temp folder\n",
    "        tempdir = tempfile.TemporaryDirectory()\n",
    "        for (i,image) in enumerate(cropped_buffer):\n",
    "            cv2.imwrite(str(tempdir) + \"%d\"%(i+1) + \".jpg\", image)     # Write lip image\n",
    "\n",
    "        # Save video\n",
    "        command = f'ffmpeg -i {str(tempdir)}%d.jpg {output_video_path}'\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "        os.removedirs(str(tempdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ROIExtractor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many faces\n",
      "Too many faces\n",
      "Too many faces\n",
      "Too many faces\n",
      "Too many faces\n",
      "done\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m extractor\u001b[39m.\u001b[39;49mprocess_video(\u001b[39m\"\u001b[39;49m\u001b[39mdataset/angel_clip_mp4_compressed.mp4\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mangel_output.mp4\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [1], line 61\u001b[0m, in \u001b[0;36mROIExtractor.process_video\u001b[0;34m(self, input_video_path, output_video_path)\u001b[0m\n\u001b[1;32m     59\u001b[0m cropped_buffer \u001b[39m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m (i,landmark) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(landmark_buffer):\n\u001b[0;32m---> 61\u001b[0m     lip_landmark \u001b[39m=\u001b[39m landmark[\u001b[39m48\u001b[39;49m:\u001b[39m68\u001b[39;49m]                                          \u001b[39m# Landmark corresponding to lip\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     lip_x \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(lip_landmark,key \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m pointx: pointx[\u001b[39m0\u001b[39m])             \u001b[39m# Lip landmark sorted for determining lip region\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     lip_y \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(lip_landmark, key \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m pointy: pointy[\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "extractor.process_video(\"dataset/angel_clip_mp4_compressed.mp4\", \"angel_output.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def shape_to_list(shape):\n",
    "\tcoords = []\n",
    "\tfor i in range(0, 68):\n",
    "\t\tcoords.append((shape.part(i).x, shape.part(i).y))\n",
    "\treturn coords\n",
    "\n",
    "def process_video(input_video_path, output_video_path, face_predictor_path):\n",
    "    landmark_detector = dlib.shape_predictor(face_predictor_path)\n",
    "\n",
    "    global detect_landmark\n",
    "\n",
    "    # Obtain face landmark information\n",
    "    def detect_landmark(image):\n",
    "        face_rects = face_detector(image,1)             # Detect face\n",
    "        if len(face_rects) < 1:                 # No face detected\n",
    "            print(\"No face detected\")\n",
    "            return\n",
    "        if len(face_rects) > 1:                  # Too many face detected\n",
    "            print(\"Too many faces\")\n",
    "            return\n",
    "        rect = face_rects[0]                    # Proper number of face\n",
    "        landmark = landmark_detector(image, rect)   # Detect face landmarks\n",
    "        landmark = shape_to_list(landmark)\n",
    "        # landmark_buffer.append(landmark)\n",
    "        return landmark\n",
    "    \n",
    "    TEMP_PATH = './process_video/'       # The path that the result images will be saved\n",
    "    LIP_MARGIN = 0.3                # Marginal rate for lip-only image.\n",
    "    RESIZE = (96,96)                # Final image size\n",
    "\n",
    "    vid = cv2.VideoCapture(input_video_path)       # Read video\n",
    "\n",
    "    # Parse into frames\n",
    "    frame_buffer = []               # A list to hold frame images\n",
    "    frame_buffer_color = []         # A list to hold original frame images\n",
    "    while(True):\n",
    "        success, frame = vid.read()                # Read frame\n",
    "        if not success:\n",
    "            break                           # Break if no frame to read left\n",
    "        gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)   # Convert image into grayscale\n",
    "        frame_buffer.append(gray)                  # Add image to the frame buffer\n",
    "        frame_buffer_color.append(frame)\n",
    "    vid.release()\n",
    "\n",
    "    with Pool(processes = 10) as pool:\n",
    "        landmark_buffer = pool.map(detect_landmark, frame_buffer)\n",
    "    \n",
    "    print(\"done\")\n",
    "\n",
    "    # Crop images\n",
    "    cropped_buffer = []\n",
    "    for (i,landmark) in enumerate(landmark_buffer):\n",
    "        lip_landmark = landmark[48:68]                                          # Landmark corresponding to lip\n",
    "        lip_x = sorted(lip_landmark,key = lambda pointx: pointx[0])             # Lip landmark sorted for determining lip region\n",
    "        lip_y = sorted(lip_landmark, key = lambda pointy: pointy[1])\n",
    "        x_add = int((-lip_x[0][0]+lip_x[-1][0])*LIP_MARGIN)                     # Determine Margins for lip-only image\n",
    "        y_add = int((-lip_y[0][1]+lip_y[-1][1])*LIP_MARGIN)\n",
    "        crop_pos = (lip_x[0][0]-x_add, lip_x[-1][0]+x_add, lip_y[0][1]-y_add, lip_y[-1][1]+y_add)   # Crop image\n",
    "        cropped = frame_buffer_color[i][crop_pos[2]:crop_pos[3],crop_pos[0]:crop_pos[1]]\n",
    "        cropped = cv2.resize(cropped,(RESIZE[0],RESIZE[1]),interpolation=cv2.INTER_CUBIC)        # Resize\n",
    "        cropped_buffer.append(cropped)\n",
    "\n",
    "    # Save result to temp folder\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        for (i,image) in enumerate(cropped_buffer):\n",
    "            cv2.imwrite(str(tempdir) + \"%d\"%(i+1) + \".jpg\", image)     # Write lip image\n",
    "\n",
    "        # Save video\n",
    "        command = f'ffmpeg -i {str(tempdir)}%d.jpg {output_video_path}'\n",
    "        subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m process_video(\u001b[39m\"\u001b[39;49m\u001b[39mdataset/angel_clip_mp4_compressed.mp4\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mangel_output.mp4\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mshape_predictor_68_face_landmarks.dat\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [21], line 74\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(input_video_path, output_video_path, face_predictor_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m     cropped_buffer\u001b[39m.\u001b[39mappend(cropped)\n\u001b[1;32m     73\u001b[0m \u001b[39m# Save result to temp folder\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[39mwith\u001b[39;00m tempfile\u001b[39m.\u001b[39mTemporaryDirectory \u001b[39mas\u001b[39;00m tempdir:\n\u001b[1;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m (i,image) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cropped_buffer):\n\u001b[1;32m     76\u001b[0m         cv2\u001b[39m.\u001b[39mimwrite(\u001b[39mstr\u001b[39m(tempdir) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m, image)     \u001b[39m# Write lip image\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "process_video(\"dataset/angel_clip_mp4_compressed.mp4\", \"angel_output.mp4\", \"shape_predictor_68_face_landmarks.dat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avhubert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fde25f2f53be29d56cbe75d31373e5a9cbe0e2ed290fb6f5ff3a78f4c56c116f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
